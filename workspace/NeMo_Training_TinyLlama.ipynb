{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b95c683e",
   "metadata": {},
   "source": [
    "# NeMo Framework - Training a large language model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af423c6",
   "metadata": {},
   "source": [
    "## Overview\n",
    "Large language model (LLM) like ChatGPT possess astonishing versatility, being able to perform tasks such as induction, programming, translation, and more, with results comparable to or even superior to human experts. To learn how to pre-train a large language model (LLM). NVIDIA has introduced NeMo Framework that is capabilities to pre-process training data, distribute training across multiple GPUs efficiently.\n",
    "\n",
    "Pre-trained language model is powerful in a variety of tasks but often lack the specialized focus needed for domain-specific applications. Therefore, to adapt the language model to a domain-specific task, fine-tuning can be employed. In this notebook, you will learn how to implement two type of tuning methods, **(1)Fine-tuning** and **(2)PEFT methods** like **LoRA** for adapting language model on specific downstream task using NVIDIA NeMo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbabfa72",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "This course covers the below sections:\n",
    "1. [Pre-training](#s1)\n",
    "    - [1.1 Download dataset](#s1.1)\n",
    "    - [1.2 Data preprocessing](#s1.2)\n",
    "    - [1.3 Download pre-trained model for continued pre-training](#s1.3)\n",
    "    - [1.4 Run pre-training](#s1.4)\n",
    "    \n",
    "    \n",
    "2. [Instruction Tuning ](#s2)\n",
    "    - [2.1 Download dataset: erhwenkuo/alpaca-data-gpt4-chinese-zhtw](#s2.1)\n",
    "    - [2.2 Split the data into train, validation and test](#s2.2)\n",
    "    - [2.3 Full parameter fine-tuning](#s2.3)\n",
    "    - [2.4. Parameter Efficient Fine-tuning](#s2.4)\n",
    "\n",
    "\n",
    "3. [Evaluation](#s3)\n",
    "\n",
    "4. [Export and Deploy a NeMo Checkpoint to TensorRT-LLM](#s4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de7ae1d7",
   "metadata": {},
   "source": [
    "## 1. Pre-training <a name='s1'></a>\n",
    "\n",
    "The initial phase of our process is concentrated on model pre-training, which serves as the primary stage for the model to acquire knowledge."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caca4a69",
   "metadata": {},
   "source": [
    "### 1.1 Download dataset <a name='s1.1'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c5955e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset('erhwenkuo/wikinews-zhtw')['train']\n",
    "dataset.to_json('./data/custom_dataset/json/wikinews-zhtw.jsonl', force_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d62c385",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preprocessing\n",
    "!mkdir -p data/custom_dataset/preprocessed\n",
    "\n",
    "!python /opt/NeMo/scripts/nlp_language_modeling/preprocess_data_for_megatron.py \\\n",
    "--input=data/custom_dataset/json/wikinews-zhtw.jsonl \\\n",
    "--json-keys=text \\\n",
    "--dataset-impl mmap \\\n",
    "--tokenizer-library=huggingface \\\n",
    "--tokenizer-type TinyLlama/TinyLlama-1.1B-Chat-v1.0 \\\n",
    "--output-prefix=data/custom_dataset/preprocessed/wikinews \\\n",
    "--append-eod"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057799cc",
   "metadata": {},
   "source": [
    "### 1.3 Download pre-trained model for continued pre-training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d6aaef",
   "metadata": {},
   "outputs": [],
   "source": [
    "%env HF_MODEL=TinyLlama/TinyLlama-1.1B-Chat-v1.0\n",
    "# %env HF_MODEL=TinyLlama-1.1B-Chat-v1.0\n",
    "\n",
    "!python /opt/NeMo/scripts/checkpoint_converters/convert_llama_hf_to_nemo.py \\\n",
    "--input_name_or_path $HF_MODEL \\\n",
    "--output_path TinyLlama-1.1B-Chat-v1.0/TinyLlama-1.1B-Chat-v1.0.nemo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6075984",
   "metadata": {},
   "source": [
    "### 1.4 Run pre-training <a name='s1.4'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833935b3-13a8-4f79-b294-e739b4076081",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update continue training script for nemo:24.05\n",
    "\n",
    "file_path = '/opt/NeMo/examples/nlp/language_modeling/megatron_gpt_continue_training.py'\n",
    "insert_line = 158\n",
    "new_line = \"        cfg.trainer.precision = None\\n\"\n",
    "\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "lines.insert(insert_line, new_line)\n",
    "\n",
    "with open(file_path, 'w', encoding='utf-8') as file:\n",
    "    file.writelines(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20380c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "%env MODEL_NAME=TinyLlama-1.1B\n",
    "%env MODEL=TinyLlama-1.1B-Chat-v1.0/TinyLlama-1.1B-Chat-v1.0.nemo\n",
    "%env NUM_GPUS=1\n",
    "%env MAX_STEPS=100\n",
    "%env MBS=1\n",
    "%env GBS=1\n",
    "%env TP=1\n",
    "%env PP=1\n",
    "%env LR=1e-4\n",
    "%env DATA_SPLITS='9990,8,2'\n",
    "%env DATA_PREFIX=[1.0,data/custom_dataset/preprocessed/wikinews_text_document]\n",
    "\n",
    "!python /opt/NeMo/examples/nlp/language_modeling/megatron_gpt_continue_training.py \\\n",
    "--config-path=/opt/NeMo-Framework-Launcher/launcher_scripts/conf/training/llama --config-name=llama2_7b \\\n",
    "+restore_from_path=$MODEL \\\n",
    "+base_results_dir=results \\\n",
    "+model.seq_len_interpolation_factor=null \\\n",
    "trainer.num_nodes=1 \\\n",
    "trainer.devices=$NUM_GPUS \\\n",
    "trainer.precision=bf16 \\\n",
    "trainer.max_steps=$MAX_STEPS \\\n",
    "trainer.limit_val_batches=32 \\\n",
    "trainer.val_check_interval=100 \\\n",
    "exp_manager.explicit_log_dir=/workspace/results/$MODEL_NAME/Pretraining \\\n",
    "exp_manager.wandb_logger_kwargs.name=$MODEL_NAME \\\n",
    "exp_manager.checkpoint_callback_params.save_nemo_on_train_end=True \\\n",
    "exp_manager.checkpoint_callback_params.model_parallel_size=$(($TP*$PP)) \\\n",
    "+exp_manager.checkpoint_callback_params.every_n_train_steps=50 \\\n",
    "+exp_manager.checkpoint_callback_params.every_n_epochs=null \\\n",
    "exp_manager.checkpoint_callback_params.monitor=\"epoch\" \\\n",
    "exp_manager.checkpoint_callback_params.save_top_k=-1 \\\n",
    "model.micro_batch_size=$MBS \\\n",
    "model.global_batch_size=$GBS \\\n",
    "model.tensor_model_parallel_size=$TP \\\n",
    "model.pipeline_model_parallel_size=$PP \\\n",
    "model.tokenizer.library=huggingface \\\n",
    "model.tokenizer.type=TinyLlama/TinyLlama-1.1B-Chat-v1.0 \\\n",
    "model.tokenizer.model=null \\\n",
    "model.optim.lr=$LR \\\n",
    "model.data.splits_string=${DATA_SPLITS} \\\n",
    "model.data.data_prefix=${DATA_PREFIX} \\\n",
    "model.data.num_workers=0 \\\n",
    "model.data.seq_length=1024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a783e2f1",
   "metadata": {},
   "source": [
    "## 2. Instruction Tuning <a name='s2'></a>\n",
    "\n",
    "We will be using the [erhwenkuo/alpaca-data-gpt4-chinese-zhtw](https://huggingface.co/datasets/erhwenkuo/alpaca-data-gpt4-chinese-zhtw) is a dataset that contains Chinese (zh-tw) Instruction-Following generated by GPT-4 using Alpaca prompts for fine-tuning LLMs.\n",
    "\n",
    "The dataset was originaly shared in [this repository](https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM). This dataset is a translation from English to Chinese.\n",
    "\n",
    "### 2.1 Download dataset: erhwenkuo/alpaca-data-gpt4-chinese-zhtw <a name='s2.1'></a>\n",
    "Let's download dataset and save it as json first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c0c8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset('erhwenkuo/alpaca-data-gpt4-chinese-zhtw')['train']\n",
    "output_path = 'data/alpaca/gpt4-chinese-zhtw.jsonl'\n",
    "os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "\n",
    "with open(output_path, 'w') as f:\n",
    "    for human_instruction, human_input, assistant_output in zip(dataset['instruction'], dataset['input'], dataset['output']):\n",
    "        f.write(json.dumps({'input': '\\n'.join([human_instruction.strip(),human_input.strip()]).strip(), 'output': assistant_output.strip()}, ensure_ascii=False)+ '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbbfe455",
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -n 1 data/alpaca/gpt4-chinese-zhtw.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482ce086",
   "metadata": {},
   "source": [
    "### 2.2 Split the data into train, validation and test. <a name='s2.2'></a>\n",
    "\n",
    "Generate the train, test and validation splits- you may use your own script to do this or create a new script and use the following sample split_train_val.py by copying it over in the chinese-dolly directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e05f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "input_file = \"data/alpaca/gpt4-chinese-zhtw.jsonl\"\n",
    "training_output_file = \"data/alpaca/training.jsonl\"\n",
    "validation_output_file = \"data/alpaca/validation.jsonl\"\n",
    "test_output_file = \"data/alpaca/test.jsonl\"\n",
    "\n",
    "# Specify the proportion of data for training and validation\n",
    "train_proportion = 0.98\n",
    "validation_proportion = 0.01\n",
    "test_proportion = 0.01\n",
    "\n",
    "# Read the JSONL file and shuffle the JSON objects\n",
    "with open(input_file, \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "    random.shuffle(lines)\n",
    "\n",
    "# Calculate split indices\n",
    "total_lines = len(lines)\n",
    "train_index = int(total_lines * train_proportion)\n",
    "val_index = int(total_lines * validation_proportion)\n",
    "\n",
    "# Distribute JSON objects into training and validation sets\n",
    "train_data = lines[:train_index]\n",
    "validation_data = lines[train_index:train_index+val_index]\n",
    "test_data = lines[train_index+val_index:]\n",
    "\n",
    "# Write JSON objects to training file\n",
    "with open(training_output_file, \"w\") as f:\n",
    "    for line in train_data:\n",
    "        f.write(line.strip() + \"\\n\")\n",
    "\n",
    "# Write JSON objects to validation file\n",
    "with open(validation_output_file, \"w\") as f:\n",
    "    for line in validation_data:\n",
    "        f.write(line.strip() + \"\\n\")\n",
    "\n",
    "# Write JSON objects to training file\n",
    "with open(test_output_file, \"w\") as f:\n",
    "    for line in test_data:\n",
    "        f.write(line.strip() + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb5dacd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What the dataset looks like after spliting\n",
    "!head -1 data/alpaca/training.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7a47e6",
   "metadata": {},
   "source": [
    "### 2.3 Full parameter fine-tuning  <a name='s2.3'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9044fde3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%env MODEL_NAME=TinyLlama-1.1B\n",
    "%env MODEL=results/TinyLlama-1.1B/Pretraining/checkpoints/megatron_llama.nemo\n",
    "%env NUM_GPUS=1\n",
    "%env MAX_STEPS=100\n",
    "%env VAL_INTERVAL=1.0\n",
    "%env GBS=16\n",
    "%env MBS=1\n",
    "%env TP=1\n",
    "%env PP=1\n",
    "%env LR=1e-4\n",
    "%env TRAIN_DS=[data/alpaca/training.jsonl]\n",
    "%env VALID_DS=[data/alpaca/validation.jsonl]\n",
    "%env TEST_DS=[data/alpaca/test.jsonl]\n",
    "%env CONCAT_SAMPLING_PROBS=[1.0]\n",
    "%env PROMPT_TEMPLATE=\"<|user|>\\n{input}</s>\\n<|assistant|>\\n{output}\"\n",
    "\n",
    "!python /opt/NeMo/examples/nlp/language_modeling/tuning/megatron_gpt_finetuning.py \\\n",
    "--config-path=/opt/NeMo/examples/nlp/language_modeling/tuning/conf --config-name=megatron_gpt_finetuning_config \\\n",
    "trainer.devices=$NUM_GPUS \\\n",
    "trainer.max_steps=$MAX_STEPS \\\n",
    "trainer.precision=bf16 \\\n",
    "trainer.val_check_interval=$VAL_INTERVAL \\\n",
    "exp_manager.explicit_log_dir=results/$MODEL_NAME/SFT \\\n",
    "exp_manager.checkpoint_callback_params.save_nemo_on_train_end=True \\\n",
    "model.tensor_model_parallel_size=$TP \\\n",
    "model.pipeline_model_parallel_size=$PP \\\n",
    "model.restore_from_path=$MODEL \\\n",
    "model.global_batch_size=$GBS \\\n",
    "model.micro_batch_size=$MBS \\\n",
    "model.data.train_ds.file_names=${TRAIN_DS} \\\n",
    "model.data.validation_ds.file_names=${VALID_DS} \\\n",
    "model.data.test_ds.file_names=${TEST_DS} \\\n",
    "model.data.train_ds.num_workers=0 \\\n",
    "model.data.validation_ds.num_workers=0 \\\n",
    "model.data.test_ds.num_workers=0 \\\n",
    "model.data.train_ds.concat_sampling_probabilities=${CONCAT_SAMPLING_PROBS} \\\n",
    "model.data.train_ds.prompt_template=\"$PROMPT_TEMPLATE\" \\\n",
    "model.data.train_ds.max_seq_length=1024 \\\n",
    "model.optim.lr=$LR \\\n",
    "model.peft.peft_scheme=null"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df427c8",
   "metadata": {},
   "source": [
    "### 2.4. Parameter Efficient Fine-tuning <a name='s2.4'></a>\n",
    "Fine-tuning language model can be computationally expensive and risk overfitting, especially with small, specialized datasets. Parameter-efficient fine-tuning methods like LoRA offer a solution. These techniques adapt the model to specific tasks by modifying only a subset of parameters, reducing computational costs and mitigating overfitting risks. In essence, LoRA enable a more efficient and targeted adaptation of large language models for specialized tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b3f0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%env MODEL_NAME=TinyLlama-1.1B\n",
    "%env MODEL=results/TinyLlama-1.1B/Pretraining/checkpoints/megatron_llama.nemo\n",
    "%env NUM_GPUS=1\n",
    "%env MAX_STEPS=10\n",
    "%env VAL_INTERVAL=1.0\n",
    "%env GBS=16\n",
    "%env MBS=1\n",
    "%env TP=1\n",
    "%env PP=1\n",
    "%env LR=1e-4\n",
    "%env TRAIN_DS=[data/alpaca/training.jsonl]\n",
    "%env VALID_DS=[data/alpaca/validation.jsonl]\n",
    "%env TEST_DS=[data/alpaca/test.jsonl]\n",
    "%env CONCAT_SAMPLING_PROBS=[1.0]\n",
    "%env PROMPT_TEMPLATE=\"<|user|>\\n{input}</s>\\n<|assistant|>\\n{output}\"\n",
    "\n",
    "!python /opt/NeMo/examples/nlp/language_modeling/tuning/megatron_gpt_finetuning.py \\\n",
    "--config-path=/opt/NeMo/examples/nlp/language_modeling/tuning/conf --config-name=megatron_gpt_finetuning_config \\\n",
    "trainer.devices=$NUM_GPUS \\\n",
    "trainer.max_steps=$MAX_STEPS \\\n",
    "trainer.precision=bf16 \\\n",
    "trainer.val_check_interval=$VAL_INTERVAL \\\n",
    "exp_manager.explicit_log_dir=/workspace/results/$MODEL_NAME/PEFT \\\n",
    "exp_manager.checkpoint_callback_params.save_nemo_on_train_end=True \\\n",
    "model.tensor_model_parallel_size=$TP \\\n",
    "model.pipeline_model_parallel_size=$PP \\\n",
    "model.restore_from_path=$MODEL \\\n",
    "model.global_batch_size=$GBS \\\n",
    "model.micro_batch_size=$MBS \\\n",
    "model.data.train_ds.file_names=${TRAIN_DS} \\\n",
    "model.data.validation_ds.file_names=${VALID_DS} \\\n",
    "model.data.test_ds.file_names=${TEST_DS} \\\n",
    "model.data.train_ds.num_workers=0 \\\n",
    "model.data.validation_ds.num_workers=0 \\\n",
    "model.data.test_ds.num_workers=0 \\\n",
    "model.data.train_ds.concat_sampling_probabilities=${CONCAT_SAMPLING_PROBS} \\\n",
    "model.data.train_ds.prompt_template=\"$PROMPT_TEMPLATE\" \\\n",
    "model.data.train_ds.max_seq_length=1024 \\\n",
    "model.optim.lr=$LR \\\n",
    "model.peft.peft_scheme=lora \\\n",
    "model.peft.lora_tuning.adapter_dim=32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331fd92e",
   "metadata": {},
   "source": [
    "## 3 Evaluation <a name='s3'></a>\n",
    "\n",
    "If you want to evaluate an SFT .nemo file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1048ec2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%env MODEL_NAME=TinyLlama-1.1B\n",
    "%env MODEL=results/TinyLlama-1.1B/SFT/checkpoints/megatron_gpt_peft_None_tuning.nemo\n",
    "%env NUM_GPUS=1\n",
    "%env TEST_DS=[data/alpaca/test.jsonl]\n",
    "%env OUTPUT=data/alpaca/prediction\n",
    "%env PROMPT_TEMPLATE=\"<|user|>\\n{input}</s>\\n<|assistant|>\\n{output}\"\n",
    "\n",
    "!python /opt/NeMo/examples/nlp/language_modeling/tuning/megatron_gpt_generate.py \\\n",
    "trainer.precision=bf16 \\\n",
    "trainer.devices=$NUM_GPUS \\\n",
    "model.restore_from_path=$MODEL \\\n",
    "model.tensor_model_parallel_size=$NUM_GPUS \\\n",
    "model.pipeline_model_parallel_size=1 \\\n",
    "model.megatron_amp_O2=True \\\n",
    "model.peft.restore_from_path=null \\\n",
    "model.data.test_ds.file_names=$TEST_DS \\\n",
    "model.data.test_ds.names=\\['alpaca_test'] \\\n",
    "model.data.test_ds.global_batch_size=32 \\\n",
    "model.data.test_ds.micro_batch_size=1 \\\n",
    "model.data.test_ds.tokens_to_generate=30 \\\n",
    "model.data.test_ds.label_key='output' \\\n",
    "model.data.test_ds.add_eos=True \\\n",
    "model.data.test_ds.add_sep=False \\\n",
    "model.data.test_ds.add_bos=False \\\n",
    "model.data.test_ds.truncation_field=\"input\" \\\n",
    "model.data.test_ds.prompt_template=\"$PROMPT_TEMPLATE\" \\\n",
    "model.data.test_ds.write_predictions_to_file=True \\\n",
    "model.data.test_ds.output_file_path_prefix=$OUTPUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32cab6ca-591b-43b5-a078-8f8c5f10008e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def modify_and_overwrite_jsonl(file_path):\n",
    "    data_list = []\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            data = json.loads(line)\n",
    "            data_list.append(data)\n",
    "    \n",
    "    with open(file_path, 'w', encoding='utf-8') as file:\n",
    "        for data in data_list:\n",
    "            json_line = json.dumps(data, ensure_ascii=False) + \"\\n\"\n",
    "            file.write(json_line)\n",
    "\n",
    "file_path = \"/workspace/data/alpaca/prediction_test_alpaca_test_inputs_preds_labels.jsonl\"\n",
    "modify_and_overwrite_jsonl(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d032e3",
   "metadata": {},
   "source": [
    "If you want to evaluate a PEFT Model, you should provide a base GPT model and a PEFT model .nemo file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64bff533",
   "metadata": {},
   "outputs": [],
   "source": [
    "%env MODEL_NAME=TinyLlama-1.1B\n",
    "%env MODEL=results/TinyLlama-1.1B/Pretraining/checkpoints/megatron_llama.nemo\n",
    "%env PEFT_MODEL=results/TinyLlama-1.1B/PEFT/checkpoints/megatron_gpt_peft_lora_tuning.nemo\n",
    "%env NUM_GPUS=1\n",
    "%env TEST_DS=[data/alpaca/test.jsonl]\n",
    "%env OUTPUT=/workspace/data/alpaca/prediction_peft\n",
    "%env PROMPT_TEMPLATE=\"<|user|>\\n{input}</s>\\n<|assistant|>\\n{output}\"\n",
    "\n",
    "!python /opt/NeMo/examples/nlp/language_modeling/tuning/megatron_gpt_generate.py \\\n",
    "trainer.precision=16 \\\n",
    "trainer.devices=$NUM_GPUS \\\n",
    "model.restore_from_path=$MODEL \\\n",
    "model.megatron_amp_O2=True \\\n",
    "model.peft.restore_from_path=$PEFT_MODEL \\\n",
    "model.peft.peft_scheme=lora \\\n",
    "model.data.test_ds.file_names=$TEST_DS \\\n",
    "model.data.test_ds.names=\\['alpaca_test'] \\\n",
    "model.data.test_ds.global_batch_size=32 \\\n",
    "model.data.test_ds.micro_batch_size=1 \\\n",
    "model.data.test_ds.tokens_to_generate=30 \\\n",
    "model.data.test_ds.label_key='output' \\\n",
    "model.data.test_ds.add_eos=True \\\n",
    "model.data.test_ds.add_sep=False \\\n",
    "model.data.test_ds.add_bos=False \\\n",
    "model.data.test_ds.truncation_field=\"input\" \\\n",
    "model.data.test_ds.prompt_template=\"$PROMPT_TEMPLATE\" \\\n",
    "model.data.test_ds.write_predictions_to_file=True \\\n",
    "model.data.test_ds.output_file_path_prefix=$OUTPUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24c3c6c-0733-45f1-8794-5c1a33303486",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"/workspace/data/alpaca/prediction_peft_test_alpaca_test_inputs_preds_labels.jsonl\"\n",
    "modify_and_overwrite_jsonl(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d98bdb0",
   "metadata": {},
   "source": [
    "## 4. Export and Deploy a NeMo Checkpoint to TensorRT-LLM <a name='s4'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f0e857-08a6-4b47-b055-7c6fff03762f",
   "metadata": {},
   "source": [
    "Open a terminal and run the following code:\n",
    "\n",
    "```sh\n",
    "python /opt/NeMo/scripts/deploy/nlp/deploy_triton.py \\\n",
    "--nemo_checkpoint results/TinyLlama-1.1B/SFT/checkpoints/megatron_gpt_peft_None_tuning.nemo \\\n",
    "--model_type llama \\\n",
    "--dtype float16 \\\n",
    "--triton_model_name TinyLlama\n",
    "```\n",
    "\n",
    "The command above launches a inference server. Keep it running and run the following cell to send a request to the server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02836ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python /opt/NeMo/scripts/deploy/nlp/query.py \\\n",
    "--url \"http://localhost:8000\" \\\n",
    "--model_name TinyLlama \\\n",
    "--prompt '<|system|>\\nYou are a helpful chatbot.</s>\\n<|user|>\\nHi, how are you?</s>\\n<|assistant|>\\n'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
