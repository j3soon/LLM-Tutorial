{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2109b78b",
   "metadata": {},
   "source": [
    "# TensorRT-LLM: Llama-3-Taiwan-8B-Instruct\n",
    "TensorRT-LLM provides users with an easy-to-use Python API to define Large Language Models (LLMs) and build TensorRT engines that contain state-of-the-art optimizations to perform inference efficiently on NVIDIA GPUs. TensorRT-LLM also contains components to create Python and C++ runtimes that execute those TensorRT engines. It also includes a backend for integration with the NVIDIA Triton Inference Server. Models built with TensorRT-LLM can be executed on a wide range of configurations going from a single GPU to multiple nodes with multiple GPUs (using Tensor Parallelism)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22dbac6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install datasets==2.19 rouge_score\n",
    "! git clone https://github.com/triton-inference-server/tensorrtllm_backend.git -b v0.9.0 --single-branch\n",
    "%cd tensorrtllm_backend/\n",
    "! git lfs install\n",
    "! git submodule update --init --recursive\n",
    "%cd /workspace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5640aed",
   "metadata": {},
   "source": [
    "### Download model from Huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c87ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!huggingface-cli download yentinglin/Llama-3-Taiwan-8B-Instruct --local-dir Llama-3-Taiwan-8B-Instruct --local-dir-use-symlinks=False "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d2910e",
   "metadata": {},
   "source": [
    "### 1. Build TensorRT-LLM engines - FP16\n",
    "\n",
    "**TensorRT-LLM** builds TensorRT engine(s) from HF checkpoint. If no checkpoint directory is specified, TensorRT-LLM will build engine(s) with dummy weights.\n",
    "\n",
    "Normally trtllm-build only requires single GPU, but if you've already got all the GPUs needed for inference, you could enable parallel building to make the engine building process faster by adding --workers argument. Please note that currently workers feature only supports single node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea209676",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model weight path, output checkpoint path and output engine path\n",
    "%env HF_LLAMA_MODEL=/workspace/Llama-3-Taiwan-8B-Instruct\n",
    "%env UNIFIED_CKPT_PATH=ckpt/llama/8b/fp16\n",
    "%env ENGINE_PATH=engines/llama/8b/fp16\n",
    "\n",
    "!python tensorrtllm_backend/tensorrt_llm/examples/llama/convert_checkpoint.py --model_dir ${HF_LLAMA_MODEL} \\\n",
    "--output_dir ${UNIFIED_CKPT_PATH} \\\n",
    "--dtype float16\n",
    "\n",
    "!trtllm-build --checkpoint_dir ${UNIFIED_CKPT_PATH} \\\n",
    "             --remove_input_padding enable \\\n",
    "             --gpt_attention_plugin float16 \\\n",
    "             --context_fmha enable \\\n",
    "             --gemm_plugin float16 \\\n",
    "             --output_dir ${ENGINE_PATH} \\\n",
    "             --paged_kv_cache enable \\\n",
    "             --max_batch_size 64"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a80524",
   "metadata": {},
   "source": [
    "### Run FP16 engine Inference\n",
    "To run a TensorRT-LLM LLaMA model using the engines generated by trtllm-build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff68231",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 tensorrtllm_backend/tensorrt_llm/examples/run.py \\\n",
    "--max_output_len 50 \\\n",
    "--tokenizer_dir ${HF_LLAMA_MODEL} \\\n",
    "--engine_dir ${ENGINE_PATH}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0653f264",
   "metadata": {},
   "source": [
    "### 2. Build TensorRT-LLM engines - INT8 KV cache + per-channel weight-only quantization\n",
    "To maximize performance and reduce memory footprint, TensorRT-LLM allows the models to be executed using different quantization modes. TensorRT-LLM supports INT4 or INT8 weights (and FP16 activations; a.k.a. INT4/INT8 weight-only) as well as a complete implementation of the SmoothQuant technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0685b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model weight path, output checkpoint path and output engine path\n",
    "%env HF_LLAMA_MODEL=/workspace/Llama-3-Taiwan-8B-Instruct\n",
    "%env UNIFIED_CKPT_PATH=ckpt/llama/8b/int8\n",
    "%env ENGINE_PATH=engines/llama/8b/int8\n",
    "\n",
    "!python tensorrtllm_backend/tensorrt_llm/examples/llama/convert_checkpoint.py --model_dir ${HF_LLAMA_MODEL} \\\n",
    "--output_dir ${UNIFIED_CKPT_PATH} \\\n",
    "--dtype float16 \\\n",
    "--int8_kv_cache \\\n",
    "--use_weight_only \\\n",
    "--weight_only_precision int8\n",
    "\n",
    "!trtllm-build --checkpoint_dir ${UNIFIED_CKPT_PATH} \\\n",
    "             --remove_input_padding enable \\\n",
    "             --gpt_attention_plugin float16 \\\n",
    "             --context_fmha enable \\\n",
    "             --gemm_plugin float16 \\\n",
    "             --output_dir ${ENGINE_PATH} \\\n",
    "             --paged_kv_cache enable \\\n",
    "             --max_batch_size 64"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161f6cfb",
   "metadata": {},
   "source": [
    "### Summarization using the LLaMA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36aba0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python tensorrtllm_backend/tensorrt_llm/examples/summarize.py \\\n",
    "--data_type fp16 \\\n",
    "--test_hf \\\n",
    "--hf_model_dir ${HF_LLAMA_MODEL} \\\n",
    "--test_trt_llm \\\n",
    "--engine_dir ${ENGINE_PATH} \\\n",
    "--max_ite 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970aeb8b",
   "metadata": {},
   "source": [
    "### 3. Build TensorRT-LLM engines - FP8 Post-Training Quantization\n",
    "\n",
    "The examples below uses the NVIDIA Modelopt (AlgorithMic Model Optimization) toolkit for the model quantization process.\n",
    "Although the V100 does not support the FP8 datatype, we have included it as a reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5458ca72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model weight path, output checkpoint path and output engine path\n",
    "# %env HF_LLAMA_MODEL=/workspace/Llama-3-Taiwan-8B-Instruct\n",
    "# %env UNIFIED_CKPT_PATH=ckpt/llama/8b/fp8\n",
    "# %env ENGINE_PATH=engines/llama/8b/fp8\n",
    "\n",
    "# !python tensorrtllm_backend/tensorrt_llm/examples/quantization/quantize.py --model_dir ${HF_LLAMA_MODEL} \\\n",
    "# --dtype float16 \\\n",
    "# --qformat fp8 \\\n",
    "# --kv_cache_dtype fp8 \\\n",
    "# --output_dir ${UNIFIED_CKPT_PATH} \\\n",
    "# --calib_size 512\n",
    "\n",
    "\n",
    "# !trtllm-build --checkpoint_dir ${UNIFIED_CKPT_PATH} \\\n",
    "#              --remove_input_padding enable \\\n",
    "#              --gpt_attention_plugin float16 \\\n",
    "#              --context_fmha enable \\\n",
    "#              --gemm_plugin float16 \\\n",
    "#              --output_dir ${ENGINE_PATH} \\\n",
    "#              --paged_kv_cache enable \\\n",
    "#              --max_batch_size 64"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74fe5eb",
   "metadata": {},
   "source": [
    "### 4. Build TensorRT-LLM engines - Groupwise quantization (AWQ/GPTQ)\n",
    "One can enable AWQ/GPTQ INT4 weight only quantization with these options when building engine with trtllm-build:\n",
    "NVIDIA Modelopt toolkit is used for AWQ weight quantization. Please see [examples/quantization/README.md](tensorrtllm_backend/tensorrt_llm/examples/quantization/README.md) for Modelopt installation instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ac44d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model weight path, output checkpoint path and output engine path\n",
    "%env HF_LLAMA_MODEL=/workspace/Llama-3-Taiwan-8B-Instruct\n",
    "%env UNIFIED_CKPT_PATH=ckpt/llama/8b/int4\n",
    "%env ENGINE_PATH=engines/llama/8b/int4\n",
    "\n",
    "# Quantize HF LLaMA 8B checkpoint into INT4 AWQ format\n",
    "!python tensorrtllm_backend/tensorrt_llm/examples/quantization/quantize.py \\\n",
    "--model_dir ${HF_LLAMA_MODEL} \\\n",
    "--dtype float16 \\\n",
    "--qformat int4_awq \\\n",
    "--awq_block_size 128 \\\n",
    "--output_dir ${UNIFIED_CKPT_PATH} \\\n",
    "--calib_size 16\n",
    "\n",
    "!trtllm-build --checkpoint_dir ${UNIFIED_CKPT_PATH} \\\n",
    "             --remove_input_padding enable \\\n",
    "             --gpt_attention_plugin float16 \\\n",
    "             --context_fmha enable \\\n",
    "             --gemm_plugin float16 \\\n",
    "             --output_dir ${ENGINE_PATH} \\\n",
    "             --paged_kv_cache enable \\\n",
    "             --max_batch_size 64"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d19a0cb",
   "metadata": {},
   "source": [
    "# Triton Inference Server with TensorRT-LLM backend: Llama-3-8B-Instruct Deployment using Triton Inference Server\n",
    "\n",
    "The Triton for [TensorRT-LLM](https://github.com/NVIDIA/TensorRT-LLM) backend. You can learn more about Triton backends in the [backend repo](https://github.com/triton-inference-server/tensorrtllm_backend/tree/main). The goal of TensorRT-LLM Backend is to let you serve TensorRT-LLM models with Triton Inference Server.\n",
    "\n",
    "## Using the TensorRT-LLM Backend\n",
    "We will look at 4 steps to serve the TensorRT-LLM model with the Triton TensorRT-LLM Backend on a 1-GPU environment. The example uses [meta-llama/Meta-Llama-3-8B-Instruct](https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct) from the TensorRT-LLM repository.\n",
    "\n",
    "### 1. Build TensorRT-LLM engines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ca9c74",
   "metadata": {},
   "source": [
    "### 2. Prepare inference configs\n",
    "\n",
    "There are four models in the all_models/inflight_batcher_llm directory that will be used in this example: preprocessing -> tensorrt_llm -> postprocessing\n",
    "\n",
    "- **preprocessing**: This model is used for tokenizing, meaning the conversion from prompts(string) to input_ids(list of ints).\n",
    "- **tensorrt_llm**: This model is a wrapper of your TensorRT-LLM model and is used for inferencing\n",
    "- **postprocessing**: This model is used for de-tokenizing, meaning the conversion from output_ids(list of ints) to outputs(string).\n",
    "- **ensemble**: This model is used to chain the three models above together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29aa6bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%env HF_LLAMA_MODEL=/workspace/Llama-3-Taiwan-8B-Instruct\n",
    "%env ENGINE_PATH=engines/llama/8b/fp16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0611c1bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%env BS=64\n",
    "!rm -rf tensorrtllm_backend/llama_ifb\n",
    "!cp -r tensorrtllm_backend/all_models/inflight_batcher_llm/ tensorrtllm_backend/llama_ifb\n",
    "\n",
    "!python3 tensorrtllm_backend/tools/fill_template.py -i tensorrtllm_backend/llama_ifb/preprocessing/config.pbtxt tokenizer_dir:${HF_LLAMA_MODEL},triton_max_batch_size:$BS,preprocessing_instance_count:1\n",
    "!python3 tensorrtllm_backend/tools/fill_template.py -i tensorrtllm_backend/llama_ifb/postprocessing/config.pbtxt tokenizer_dir:${HF_LLAMA_MODEL},triton_max_batch_size:$BS,postprocessing_instance_count:1\n",
    "!python3 tensorrtllm_backend/tools/fill_template.py -i tensorrtllm_backend/llama_ifb/tensorrt_llm_bls/config.pbtxt triton_max_batch_size:$BS,decoupled_mode:False,bls_instance_count:1,accumulate_tokens:False\n",
    "!python3 tensorrtllm_backend/tools/fill_template.py -i tensorrtllm_backend/llama_ifb/ensemble/config.pbtxt triton_max_batch_size:$BS\n",
    "!python3 tensorrtllm_backend/tools/fill_template.py -i tensorrtllm_backend/llama_ifb/tensorrt_llm/config.pbtxt triton_backend:tensorrtllm,triton_max_batch_size:$BS,decoupled_mode:False,max_beam_width:1,engine_dir:${ENGINE_PATH},max_tokens_in_paged_kv_cache:2560,max_attention_window_size:2560,kv_cache_free_gpu_mem_fraction:0.5,exclude_input_in_output:True,enable_kv_cache_reuse:False,batching_strategy:inflight_fused_batching,max_queue_delay_microseconds:0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf6a38e",
   "metadata": {},
   "source": [
    "### 3. Launch Triton server\n",
    "Open a terminal and run the following code:\n",
    "```bash\n",
    "cd /workspace/\n",
    "python /workspace/tensorrtllm_backend/scripts/launch_triton_server.py --world_size 1 --model_repo=/workspace/tensorrtllm_backend/llama_ifb/\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17065c70",
   "metadata": {},
   "source": [
    "## Query the server with the Triton-generated endpoint\n",
    "You can query the server using Triton's generate endpoint with a curl command based on the following general format within your client environment/container:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7948571",
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -X POST localhost:8000/v2/models/ensemble/generate -d \\\n",
    "'{\"text_input\": \"What is machine learning?\", \\\n",
    "\"max_tokens\": 20, \\\n",
    "\"bad_words\": \"\", \\\n",
    "\"stop_words\": \"\", \\\n",
    "\"pad_id\": 2, \\\n",
    "\"end_id\": 2}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320b5c74",
   "metadata": {},
   "source": [
    "## Querying and Formatting using Python\n",
    "We notice the format is not quite useful, let us now try to do the same via Python, here is a snippet in Python that does the same as above, let us run it now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51053c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Retrieve the HTTP port from environment variables\n",
    "http_port = 8000\n",
    "\n",
    "# Check if HTTP_PORT is set\n",
    "if http_port is None:\n",
    "    print(\"Error: HTTP_PORT environment variable is not set.\")\n",
    "    exit(1)\n",
    "\n",
    "# Set the URL with the HTTP port\n",
    "url = f'http://localhost:{http_port}/v2/models/ensemble/generate'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466dd50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the payload\n",
    "input_text = \"What is machine learning?\"\n",
    "payload = {\n",
    "    \"text_input\": input_text,\n",
    "    \"max_tokens\": 1024,\n",
    "    \"bad_words\": \"\",\n",
    "    \"stop_words\": \"<|eot_id|>\"\n",
    "}\n",
    "\n",
    "# Make a POST request\n",
    "response = requests.post(url, json=payload)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Parse the response\n",
    "    data = response.json()\n",
    "    output_text = data.get('text_output')\n",
    "\n",
    "    # Format and print the output\n",
    "    print(f\"Input: {input_text}\")\n",
    "    print(f\"Output: {output_text}\")\n",
    "else:\n",
    "    print(f\"Error: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f0671e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the payload\n",
    "input_text = \"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\\n",
    "You are a helpful AI assistant<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\\n",
    "What is machine learning?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\"\n",
    "\n",
    "payload = {\n",
    "    \"text_input\": input_text,\n",
    "    \"max_tokens\": 1024,\n",
    "    \"bad_words\": \"\",\n",
    "    \"stop_words\": \"<|eot_id|>\"\n",
    "}\n",
    "\n",
    "# Make a POST request\n",
    "response = requests.post(url, json=payload)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Parse the response\n",
    "    data = response.json()\n",
    "    output_text = data.get('text_output')\n",
    "\n",
    "    # Format and print the output\n",
    "    print(f\"Input: {input_text}\")\n",
    "    print(f\"Output: {output_text}\")\n",
    "else:\n",
    "    print(f\"Error: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17eaa4d7",
   "metadata": {},
   "source": [
    "## Kill the server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9674ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pgrep mpirun | xargs kill"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
