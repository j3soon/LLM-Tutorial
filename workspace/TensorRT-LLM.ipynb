{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27af7c62",
   "metadata": {},
   "source": [
    "# TensorRT-LLM: Llama-3-Taiwan-8B-Instruct\n",
    "The objective of this notebook is to demonstrate the use of TensorRT-LLM to optimize Llama-3-Instruct, run inference, and examine using various advance optimization techniques.\n",
    "\n",
    "## Overview of TensorRT-LLM\n",
    "\n",
    "TensorRT-LLM provides users with an easy-to-use Python API to define Large Language Models (LLMs) and build TensorRT engines that contain state-of-the-art optimizations to perform inference efficiently on NVIDIA GPUs. TensorRT-LLM also contains components to create Python and C++ runtimes that execute those TensorRT engines. It also includes a backend for integration with the NVIDIA Triton Inference Server. Models built with TensorRT-LLM can be executed on a wide range of configurations going from a single GPU to multiple nodes with multiple GPUs (using Tensor Parallelism)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48a86cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install datasets==2.19 rouge_score\n",
    "! git clone https://github.com/triton-inference-server/tensorrtllm_backend.git -b v0.9.0 --single-branch\n",
    "%cd tensorrtllm_backend/\n",
    "! git lfs install\n",
    "! git submodule update --init --recursive\n",
    "%cd /workspace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92dc35a",
   "metadata": {},
   "source": [
    "## 1. Download model from Huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73fcf797",
   "metadata": {},
   "outputs": [],
   "source": [
    "!huggingface-cli download yentinglin/Llama-3-Taiwan-8B-Instruct --local-dir Llama-3-Taiwan-8B-Instruct --local-dir-use-symlinks=False "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24e219a",
   "metadata": {},
   "source": [
    "## 2. Building TensorRT-LLM engine(s) for Llama-3-Taiwan-8B-Instruct\n",
    "\n",
    "This section shows how to build tensorrt engine(s) using huggingface model.\n",
    "Before we proceed to build our engine, it is important to be aware of the supported matrixes for Llama-3 as listed below:\n",
    "\n",
    "- FP16\n",
    "- FP8\n",
    "- INT8 & INT4 Weight-Only\n",
    "- SmoothQuant\n",
    "- Groupwise quantization (AWQ/GPTQ)\n",
    "- FP8 KV cache\n",
    "- INT8 KV cache (+ AWQ/per-channel weight-only)\n",
    "- Tensor Parallel\n",
    "\n",
    "### 2.1 Build TensorRT-LLM engines - FP16\n",
    "\n",
    "**TensorRT-LLM** builds TensorRT engine(s) from HF checkpoint. Firstly, we used the `convert_checkpoint.py` script to convert Llama-3-Taiwan-8B-Instruct into tensorrt-llm checkpoint format. We use the `trtllm-build` command to build our tensorrt engine.\n",
    "\n",
    "The `trtllm-build` command builds TensorRT-LLM engines from TensorRT-LLM checkpoints. The checkpoint directory provides the model's weights and architecture configuration. The number of engine files is also same to the number of GPUs used to run inference.\n",
    "\n",
    "`trtllm-build` command has a variety of options. In particular, the plugin-related options have two categories:\n",
    "\n",
    "- Plugin options that requires a data type (e.g., `gpt_attention_plugin`), you can\n",
    "    - explicitly specify `float16`/`bfloat16`/`float32`, so that the plugins are enabled with the specified precision;\n",
    "    - implicitly specify `auto`, so that the plugins are enabled with the precision automatically inferred from model dtype (i.e., the dtype specified in weight conversion); or\n",
    "    - disable the plugin by `disable`.\n",
    "    \n",
    "- Other features that requires a boolean (e.g., `context_fmha`, `paged_kv_cache`, `remove_input_padding`), you can\n",
    "enable/disable the feature by specifying `enable`/`disable`.\n",
    "\n",
    "Normally `trtllm-build` only requires single GPU, but if you've already got all the GPUs needed for inference, you could enable parallel building to make the engine building process faster by adding --workers argument. Please note that currently workers feature only supports single node.\n",
    "\n",
    "The last step is to run the inference using the `run.py` and `summarize.py` script. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d81687",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model weight path, output checkpoint path and output engine path\n",
    "%env HF_LLAMA_MODEL=/workspace/Llama-3-Taiwan-8B-Instruct\n",
    "%env UNIFIED_CKPT_PATH=ckpt/llama/8b/fp16\n",
    "%env ENGINE_PATH=engines/llama/8b/fp16\n",
    "\n",
    "!python tensorrtllm_backend/tensorrt_llm/examples/llama/convert_checkpoint.py \\\n",
    "--model_dir ${HF_LLAMA_MODEL} \\\n",
    "--output_dir ${UNIFIED_CKPT_PATH} \\\n",
    "--dtype float16\n",
    "\n",
    "!trtllm-build --checkpoint_dir ${UNIFIED_CKPT_PATH} \\\n",
    "             --remove_input_padding enable \\\n",
    "             --gpt_attention_plugin float16 \\\n",
    "             --gemm_plugin float16 \\\n",
    "             --output_dir ${ENGINE_PATH}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f5d329",
   "metadata": {},
   "source": [
    "#### flag description for `convert_checkpoint.py`:\n",
    "- `model_dir`: path to the model directory\n",
    "- `output_dir`: path to the directory to store the tensorrt-llm checkpoint format or the tensorrt engine\n",
    "- `dtype`: data type to use for model conversion to tensorrt-llm checkpoint\n",
    "\n",
    "#### flag description for `trtllm-build`:\n",
    "- `checkpoint_dir`: path to the directory to load the tensorrt-llm checkpoint needed to build the tensorrt engine\n",
    "- `gpt_attention_plugin`: GPT attention plugin\n",
    "- `gemm_plugin`: required plugin to prevent accuracy issue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee70edc",
   "metadata": {},
   "source": [
    "### Run FP16 engine Inference\n",
    "To run a TensorRT-LLM LLaMA model using the engines generated by trtllm-build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5d2315",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 tensorrtllm_backend/tensorrt_llm/examples/run.py \\\n",
    "--max_output_len 50 \\\n",
    "--tokenizer_dir ${HF_LLAMA_MODEL} \\\n",
    "--engine_dir ${ENGINE_PATH}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6667c412",
   "metadata": {},
   "source": [
    "### 2.2 Build TensorRT-LLM engines - INT8 KV cache + per-channel weight-only quantization\n",
    "To maximize performance and reduce memory footprint, TensorRT-LLM allows the models to be executed using different quantization modes. TensorRT-LLM supports INT4 or INT8 weights (and FP16 activations; a.k.a. INT4/INT8 weight-only) as well as a complete implementation of the SmoothQuant technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f6d28e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model weight path, output checkpoint path and output engine path\n",
    "%env HF_LLAMA_MODEL=/workspace/Llama-3-Taiwan-8B-Instruct\n",
    "%env UNIFIED_CKPT_PATH=ckpt/llama/8b/int8\n",
    "%env ENGINE_PATH=engines/llama/8b/int8\n",
    "\n",
    "!python tensorrtllm_backend/tensorrt_llm/examples/llama/convert_checkpoint.py \\\n",
    "--model_dir ${HF_LLAMA_MODEL} \\\n",
    "--output_dir ${UNIFIED_CKPT_PATH} \\\n",
    "--dtype float16 \\\n",
    "--use_weight_only \\\n",
    "--weight_only_precision int8\n",
    "\n",
    "!trtllm-build --checkpoint_dir ${UNIFIED_CKPT_PATH} \\\n",
    "             --remove_input_padding enable \\\n",
    "             --gpt_attention_plugin float16 \\\n",
    "             --gemm_plugin float16 \\\n",
    "             --output_dir ${ENGINE_PATH}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff0e47c",
   "metadata": {},
   "source": [
    "### Summarization using the LLaMA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122d9c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python tensorrtllm_backend/tensorrt_llm/examples/summarize.py \\\n",
    "--data_type fp16 \\\n",
    "--test_hf \\\n",
    "--hf_model_dir ${HF_LLAMA_MODEL} \\\n",
    "--test_trt_llm \\\n",
    "--engine_dir ${ENGINE_PATH} \\\n",
    "--max_ite 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa30e439",
   "metadata": {},
   "source": [
    "### 2.3 Build TensorRT-LLM engines - FP8 Post-Training Quantization [Optional]\n",
    "\n",
    "The examples below uses the NVIDIA Modelopt (AlgorithMic Model Optimization) toolkit for the model quantization process. Although the V100 does not support the FP8 datatype, we have included it as a reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbbbcf2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model weight path, output checkpoint path and output engine path\n",
    "# %env HF_LLAMA_MODEL=/workspace/Llama-3-Taiwan-8B-Instruct\n",
    "# %env UNIFIED_CKPT_PATH=ckpt/llama/8b/fp8\n",
    "# %env ENGINE_PATH=engines/llama/8b/fp8\n",
    "\n",
    "# !python tensorrtllm_backend/tensorrt_llm/examples/quantization/quantize.py \\\n",
    "# --model_dir ${HF_LLAMA_MODEL} \\\n",
    "# --dtype float16 \\\n",
    "# --qformat fp8 \\\n",
    "# --kv_cache_dtype fp8 \\\n",
    "# --output_dir ${UNIFIED_CKPT_PATH} \\\n",
    "# --calib_size 512\n",
    "\n",
    "\n",
    "# !trtllm-build --checkpoint_dir ${UNIFIED_CKPT_PATH} \\\n",
    "#              --remove_input_padding enable \\\n",
    "#              --gpt_attention_plugin float16 \\\n",
    "#              --gemm_plugin float16 \\\n",
    "#              --output_dir ${ENGINE_PATH}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "321c7c40",
   "metadata": {},
   "source": [
    "### 4. Build TensorRT-LLM engines - Groupwise quantization (AWQ/GPTQ)\n",
    "One can enable AWQ/GPTQ INT4 weight only quantization with these options when building engine with trtllm-build:\n",
    "NVIDIA Modelopt toolkit is used for AWQ weight quantization. Please see [examples/quantization/README.md](tensorrtllm_backend/tensorrt_llm/examples/quantization/README.md) for Modelopt installation instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4bb6311",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model weight path, output checkpoint path and output engine path\n",
    "%env HF_LLAMA_MODEL=/workspace/Llama-3-Taiwan-8B-Instruct\n",
    "%env UNIFIED_CKPT_PATH=ckpt/llama/8b/int4\n",
    "%env ENGINE_PATH=engines/llama/8b/int4\n",
    "\n",
    "# Quantize HF LLaMA 8B checkpoint into INT4 AWQ format\n",
    "!python tensorrtllm_backend/tensorrt_llm/examples/quantization/quantize.py \\\n",
    "--model_dir ${HF_LLAMA_MODEL} \\\n",
    "--dtype float16 \\\n",
    "--qformat int4_awq \\\n",
    "--awq_block_size 128 \\\n",
    "--output_dir ${UNIFIED_CKPT_PATH} \\\n",
    "--calib_size 4\n",
    "\n",
    "!trtllm-build --checkpoint_dir ${UNIFIED_CKPT_PATH} \\\n",
    "             --remove_input_padding enable \\\n",
    "             --gpt_attention_plugin float16 \\\n",
    "             --gemm_plugin float16 \\\n",
    "             --output_dir ${ENGINE_PATH} \\\n",
    "             --paged_kv_cache enable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433e679c",
   "metadata": {},
   "source": [
    "# Triton Inference Server with TensorRT-LLM backend: Llama-3-Taiwan-8B-Instruct Deployment using Triton Inference Server\n",
    "\n",
    "The Triton for [TensorRT-LLM](https://github.com/NVIDIA/TensorRT-LLM) backend. You can learn more about Triton backends in the [backend repo](https://github.com/triton-inference-server/tensorrtllm_backend/tree/main). The goal of TensorRT-LLM Backend is to let you serve TensorRT-LLM models with Triton Inference Server.\n",
    "\n",
    "## Using the TensorRT-LLM Backend\n",
    "We will look at 4 steps to serve the TensorRT-LLM model with the Triton TensorRT-LLM Backend on a 1-GPU environment. The example uses [Llama-3-Taiwan-8B-Instruct](https://huggingface.co/yentinglin/Llama-3-Taiwan-8B-Instruct) from the TensorRT-LLM repository.\n",
    "\n",
    "### 1. Build TensorRT-LLM engines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eebd6cb3",
   "metadata": {},
   "source": [
    "### 2. Prepare inference configs\n",
    "\n",
    "There are four models in the all_models/inflight_batcher_llm directory that will be used in this example: preprocessing -> tensorrt_llm -> postprocessing\n",
    "\n",
    "- **preprocessing**: This model is used for tokenizing, meaning the conversion from prompts(string) to input_ids(list of ints).\n",
    "- **tensorrt_llm**: This model is a wrapper of your TensorRT-LLM model and is used for inferencing\n",
    "- **postprocessing**: This model is used for de-tokenizing, meaning the conversion from output_ids(list of ints) to outputs(string).\n",
    "- **ensemble**: This model is used to chain the three models above together.\n",
    "\n",
    "<div><center>\n",
    "<img src=\"./images/ensemble.png\" width=\"1000\"/>\n",
    "</center></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641f1f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "%env HF_LLAMA_MODEL=/workspace/Llama-3-Taiwan-8B-Instruct\n",
    "%env ENGINE_PATH=engines/llama/8b/int8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7144744",
   "metadata": {},
   "outputs": [],
   "source": [
    "%env BS=64\n",
    "!rm -rf tensorrtllm_backend/llama_ifb\n",
    "!cp -r tensorrtllm_backend/all_models/inflight_batcher_llm/ tensorrtllm_backend/llama_ifb\n",
    "\n",
    "!python3 tensorrtllm_backend/tools/fill_template.py -i tensorrtllm_backend/llama_ifb/preprocessing/config.pbtxt tokenizer_dir:${HF_LLAMA_MODEL},triton_max_batch_size:$BS,preprocessing_instance_count:1\n",
    "!python3 tensorrtllm_backend/tools/fill_template.py -i tensorrtllm_backend/llama_ifb/postprocessing/config.pbtxt tokenizer_dir:${HF_LLAMA_MODEL},triton_max_batch_size:$BS,postprocessing_instance_count:1\n",
    "!python3 tensorrtllm_backend/tools/fill_template.py -i tensorrtllm_backend/llama_ifb/tensorrt_llm_bls/config.pbtxt triton_max_batch_size:$BS,decoupled_mode:False,bls_instance_count:1,accumulate_tokens:False\n",
    "!python3 tensorrtllm_backend/tools/fill_template.py -i tensorrtllm_backend/llama_ifb/ensemble/config.pbtxt triton_max_batch_size:$BS\n",
    "!python3 tensorrtllm_backend/tools/fill_template.py -i tensorrtllm_backend/llama_ifb/tensorrt_llm/config.pbtxt triton_backend:tensorrtllm,triton_max_batch_size:$BS,decoupled_mode:False,max_beam_width:1,engine_dir:${ENGINE_PATH},max_tokens_in_paged_kv_cache:2560,max_attention_window_size:2560,kv_cache_free_gpu_mem_fraction:0.5,exclude_input_in_output:True,enable_kv_cache_reuse:False,batching_strategy:inflight_fused_batching,max_queue_delay_microseconds:0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c4abb2",
   "metadata": {},
   "source": [
    "You can look at the `config.pbtxt` files for your reference and also learn more about the [model configuration parameters](https://github.com/triton-inference-server/tensorrtllm_backend/tree/main?tab=readme-ov-file#modify-the-model-configuration).\n",
    "\n",
    "\n",
    "\n",
    "- a) View changes in the **Pre-processing config file** *[tensorrtllm_backend/llama_ifb/preprocessing/config.pbtxt](./tensorrtllm_backend/llama_ifb/preprocessing/config.pbtxt)*\n",
    "\n",
    "\n",
    "|  Line   |Parameters | Value | \n",
    "|-|-|-| \n",
    "|   124   | `tokenizer_dir` | **`/workspace/Llama-3-Taiwan-8B-Instruct`**|\n",
    "|   29    | `triton_max_batch_size` |64|\n",
    "|   137   | `preprocessing_instance_count` | 1|\n",
    "\n",
    "---\n",
    "\n",
    "- b) View changes in the **Post-processing config file**  *[tensorrtllm_backend/llama_ifb/postprocessing/config.pbtxt](./tensorrtllm_backend/llama_ifb/postprocessing/config.pbtxt)*\n",
    "\n",
    "\n",
    "\n",
    "|  Line   | Parameters | Value | \n",
    "|-|-|-| \n",
    "|   97    | `tokenizer_dir` | **`/workspace/Llama-3-Taiwan-8B-Instruct`**|\n",
    "|   29    | `triton_max_batch_size` |64|\n",
    "|   110   | `preprocessing_instance_count` | 1|\n",
    "\n",
    "---\n",
    "\n",
    "- c) View changes in the **tensorrt_llm_bls config file**  *[tensorrtllm_backend/llama_ifb/tensorrt_llm_bls/config.pbtxt](./tensorrtllm_backend/llama_ifb/tensorrt_llm_bls/config.pbtxt)*\n",
    "\n",
    "\n",
    "|  Line   | Parameters | Value | \n",
    "|-|-|-| \n",
    "|   29    | `triton_max_batch_size` | 64|\n",
    "|   32    | `decoupled_mode` |False|\n",
    "|   244   | `bls_instance_count` | 1|\n",
    "|   226   | `accumulate_tokens` |False|\n",
    "\n",
    "\n",
    "d) View changes in the **Ensemble config file**  *[tensorrtllm_backend/llama_ifb/ensemble/config.pbtxt](./tensorrtllm_backend/llama_ifb/ensemble/config.pbtxt)*\n",
    "\n",
    " \n",
    "|  Line   | Parameters | Value|\n",
    "|-|-|-|\n",
    "|    29   | `triton_max_batch_size` |64 |\n",
    "\n",
    "---\n",
    "\n",
    "- e)  View changes in the **tensorrt_llm config file**  *[tensorrtllm_backend/llama_ifb/tensorrt_llm/config.pbtxt](./tensorrtllm_backend/llama_ifb/tensorrt_llm/config.pbtxt)*\n",
    "\n",
    "\n",
    "|  Line   | Name | Value|\n",
    "|-|-|-|\n",
    "|   28    |  `triton_backend`        |    \"tensorrtllm\"                |\n",
    "|   29    |`triton_max_batch_size` | 64 |\n",
    "|   32    |`decoupled_mode` | False|\n",
    "|   350   |`max_beam_width` | 1 |\n",
    "|   368   |`engine_dir` |  **`engines/llama/8b/int4`** |\n",
    "|   374   |`max_tokens_in_paged_kv_cache` | 2560|\n",
    "|   380   |max_attention_window_size|2560|\n",
    "|   398   |kv_cache_free_gpu_mem_fraction |0.5 |\n",
    "|   423   |exclude_input_in_output |True |\n",
    "|   453   |enable_kv_cache_reuse | False|\n",
    "|   362   |batching_strategy |inflight_fused_batching |\n",
    "|    37   |max_queue_delay_microseconds | 0|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c8c631",
   "metadata": {},
   "source": [
    "### 3. Launch Triton server\n",
    "\n",
    "Open a terminal and run the following code:\n",
    "\n",
    "- On the terminal, navigate to the launch script folder by running this command:\n",
    "\n",
    "`cd /workspace/`\n",
    "\n",
    "- Start the Triton Server with this command:\n",
    "\n",
    "`python /workspace/tensorrtllm_backend/scripts/launch_triton_server.py --world_size 1 --model_repo=/workspace/tensorrtllm_backend/llama_ifb/`\n",
    "\n",
    "<center><img src=\"./images/terminal.png\"  alt-text=\"terminal\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7bc5297",
   "metadata": {},
   "source": [
    "## Query the server with the Triton-generated endpoint\n",
    "You can query the server using Triton's generate endpoint with a curl command based on the following general format within your client environment/container:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e7dcc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -X POST localhost:8000/v2/models/ensemble/generate -d \\\n",
    "'{\"text_input\": \"What is machine learning?\", \\\n",
    "\"max_tokens\": 20, \\\n",
    "\"bad_words\": \"\", \\\n",
    "\"stop_words\": \"\", \\\n",
    "\"pad_id\": 2, \\\n",
    "\"end_id\": 2}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c78e48e5",
   "metadata": {},
   "source": [
    "## Querying and Formatting using Python\n",
    "We notice the format is not quite useful, let us now try to do the same via Python, here is a snippet in Python that does the same as above, let us run it now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5381c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Retrieve the HTTP port from environment variables\n",
    "http_port = 8000\n",
    "\n",
    "# Check if HTTP_PORT is set\n",
    "if http_port is None:\n",
    "    print(\"Error: HTTP_PORT environment variable is not set.\")\n",
    "    exit(1)\n",
    "\n",
    "# Set the URL with the HTTP port\n",
    "url = f'http://localhost:{http_port}/v2/models/ensemble/generate'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991683e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the payload\n",
    "input_text = \"What is machine learning?\"\n",
    "payload = {\n",
    "    \"text_input\": input_text,\n",
    "    \"max_tokens\": 1024,\n",
    "    \"bad_words\": \"\",\n",
    "    \"stop_words\": \"<|eot_id|>\"\n",
    "}\n",
    "\n",
    "# Make a POST request\n",
    "response = requests.post(url, json=payload)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Parse the response\n",
    "    data = response.json()\n",
    "    output_text = data.get('text_output')\n",
    "\n",
    "    # Format and print the output\n",
    "    print(f\"Input: {input_text}\")\n",
    "    print(f\"Output: {output_text}\")\n",
    "else:\n",
    "    print(f\"Error: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb43a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the payload\n",
    "input_text = \"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\\n",
    "You are a helpful AI assistant<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\\n",
    "What is machine learning?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\"\n",
    "\n",
    "payload = {\n",
    "    \"text_input\": input_text,\n",
    "    \"max_tokens\": 1024,\n",
    "    \"bad_words\": \"\",\n",
    "    \"stop_words\": \"<|eot_id|>\"\n",
    "}\n",
    "\n",
    "# Make a POST request\n",
    "response = requests.post(url, json=payload)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Parse the response\n",
    "    data = response.json()\n",
    "    output_text = data.get('text_output')\n",
    "\n",
    "    # Format and print the output\n",
    "    print(f\"Input: {input_text}\")\n",
    "    print(f\"Output: {output_text}\")\n",
    "else:\n",
    "    print(f\"Error: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f500db",
   "metadata": {},
   "source": [
    "## Kill the server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6ee519",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pgrep mpirun | xargs kill"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59bbfd96",
   "metadata": {},
   "source": [
    "## Clear your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28591f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf Llama-3-Taiwan-8B-Instruct/ ckpt engines tensorrtllm_backend"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
